{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average finish time of criminal justice task: 575.3333333333334\n",
      "The average finish time of toxic comments task: 596.6666666666666\n",
      "The average number of don't know answers: 2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#Loading data\n",
    "data = pd.read_csv(\"../data/example.csv\")\n",
    "\n",
    "race = data[\"race\"]\n",
    "religion = data[\"religion\"]\n",
    "country = data[\"country\"]\n",
    "cr_t = data[\"time_criminal\"]\n",
    "toxic_t = data[\"time_toxic\"]\n",
    "tests = []\n",
    "tests.append(data[\"test_1\"])\n",
    "tests.append(data[\"test_2\"])\n",
    "tests.append(data[\"test_3\"])\n",
    "tests.append(data[\"test_4\"])\n",
    "tests.append(data[\"test_5\"])\n",
    "tests.append(data[\"test_6\"])\n",
    "tests.append(data[\"test_7\"])\n",
    "\n",
    "#Filtering out test according to the quality control\n",
    "#The test with the golden question\n",
    "golden_test = 3\n",
    "#The tests that are shuffled version of the same models\n",
    "same_model_1 = 0\n",
    "same_model_2 = 5\n",
    "\n",
    "num_tests = len(tests)\n",
    "num_data = len(tests[0])\n",
    "\n",
    "filtered_tests = []\n",
    "for i in range(num_tests):\n",
    "    filtered_tests.append([])\n",
    "    \n",
    "for i in range(num_data):\n",
    "    if tests[golden_test][i] == 5 and tests[same_model_1][i] == tests[same_model_2][i]:\n",
    "        for j in range(num_tests):\n",
    "            filtered_tests[j].append(tests[j][i]) \n",
    " \n",
    "#print(filtered_tests)\n",
    "#Calculating task times    \n",
    "tests = filtered_tests\n",
    "num_data = len(tests[0])\n",
    "\n",
    "avg_cr_t = 0\n",
    "for t in cr_t:\n",
    "    avg_cr_t += t\n",
    "avg_cr_t = avg_cr_t / len(cr_t)\n",
    "\n",
    "avg_toxic_t = 0\n",
    "for t in toxic_t:\n",
    "    avg_toxic_t += t\n",
    "avg_toxic_t = avg_toxic_t / len(toxic_t)\n",
    "\n",
    "print(\"The average finish time of criminal justice task: \" + str(avg_cr_t))\n",
    "print(\"The average finish time of toxic comments task: \" + str(avg_toxic_t))\n",
    "\n",
    "#Calculating the number of don't know answers\n",
    "counts = [0] * num_data\n",
    "for i in range(num_tests):\n",
    "    for j in range(num_data):\n",
    "        if tests[i][j] == 3:\n",
    "            counts[j] += 1\n",
    "\n",
    "avg_dont_know = 0\n",
    "for j in range(num_data):\n",
    "    avg_dont_know += counts[j]\n",
    "    \n",
    "avg_dont_know = avg_dont_know / num_data\n",
    "\n",
    "print(\"The average number of don't know answers: \" + str(avg_dont_know))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
